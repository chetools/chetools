{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "least_squares_scale_jvp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOidyp7G9T5pAFPMKZoC9xY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/profteachkids/chetools/blob/main/least_squares_scale_jvp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "from scipy.linalg import svd, qr\n",
        "from scipy.sparse.linalg import lsmr\n",
        "from scipy.optimize import OptimizeResult\n",
        "\n",
        "from scipy.optimize._lsq.common import (\n",
        "    step_size_to_bound, find_active_constraints, in_bounds,\n",
        "    make_strictly_feasible, intersect_trust_region, solve_lsq_trust_region,\n",
        "    solve_trust_region_2d, minimize_quadratic_1d, build_quadratic_1d,\n",
        "    evaluate_quadratic, right_multiplied_operator, regularized_lsq_operator,\n",
        "    CL_scaling_vector, compute_grad, check_termination,\n",
        "    update_tr_radius, scale_for_robust_loss_function, print_header_nonlinear,\n",
        "    print_iteration_nonlinear)\n",
        "\n",
        "\n",
        "def compute_jac_scale(scale_inv, scale_inv_old=None):\n",
        "    if scale_inv_old is None:\n",
        "        scale_inv[scale_inv == 0] = 1.\n",
        "    else:\n",
        "        scale_inv = np.maximum(scale_inv, scale_inv_old)\n",
        "\n",
        "    return 1 / scale_inv, scale_inv\n",
        "\n",
        "def trf(fun, jac, x0, f0, J0, lb, ub, ftol, xtol, gtol, max_nfev, x_scale,\n",
        "        loss_function, tr_solver, tr_options, verbose, scale_inv_f):\n",
        "    # For efficiency, it makes sense to run the simplified version of the\n",
        "    # algorithm when no bounds are imposed. We decided to write the two\n",
        "    # separate functions. It violates the DRY principle, but the individual\n",
        "    # functions are kept the most readable.\n",
        "    if np.all(lb == -np.inf) and np.all(ub == np.inf):\n",
        "        return trf_no_bounds(\n",
        "            fun, jac, x0, f0, J0, ftol, xtol, gtol, max_nfev, x_scale,\n",
        "            loss_function, tr_solver, tr_options, verbose, scale_inv_f)\n",
        "    else:\n",
        "        return trf_bounds(\n",
        "            fun, jac, x0, f0, J0, lb, ub, ftol, xtol, gtol, max_nfev, x_scale,\n",
        "            loss_function, tr_solver, tr_options, verbose, scale_inv_f)\n",
        "\n",
        "\n",
        "def select_step(x, J_h, diag_h, g_h, p, p_h, d, Delta, lb, ub, theta):\n",
        "    \"\"\"Select the best step according to Trust Region Reflective algorithm.\"\"\"\n",
        "    if in_bounds(x + p, lb, ub):\n",
        "        p_value = evaluate_quadratic(J_h, g_h, p_h, diag=diag_h)\n",
        "        return p, p_h, -p_value\n",
        "\n",
        "    p_stride, hits = step_size_to_bound(x, p, lb, ub)\n",
        "\n",
        "    # Compute the reflected direction.\n",
        "    r_h = np.copy(p_h)\n",
        "    r_h[hits.astype(bool)] *= -1\n",
        "    r = d * r_h\n",
        "\n",
        "    # Restrict trust-region step, such that it hits the bound.\n",
        "    p *= p_stride\n",
        "    p_h *= p_stride\n",
        "    x_on_bound = x + p\n",
        "\n",
        "    # Reflected direction will cross first either feasible region or trust\n",
        "    # region boundary.\n",
        "    _, to_tr = intersect_trust_region(p_h, r_h, Delta)\n",
        "    to_bound, _ = step_size_to_bound(x_on_bound, r, lb, ub)\n",
        "\n",
        "    # Find lower and upper bounds on a step size along the reflected\n",
        "    # direction, considering the strict feasibility requirement. There is no\n",
        "    # single correct way to do that, the chosen approach seems to work best\n",
        "    # on test problems.\n",
        "    r_stride = min(to_bound, to_tr)\n",
        "    if r_stride > 0:\n",
        "        r_stride_l = (1 - theta) * p_stride / r_stride\n",
        "        if r_stride == to_bound:\n",
        "            r_stride_u = theta * to_bound\n",
        "        else:\n",
        "            r_stride_u = to_tr\n",
        "    else:\n",
        "        r_stride_l = 0\n",
        "        r_stride_u = -1\n",
        "\n",
        "    # Check if reflection step is available.\n",
        "    if r_stride_l <= r_stride_u:\n",
        "        a, b, c = build_quadratic_1d(J_h, g_h, r_h, s0=p_h, diag=diag_h)\n",
        "        r_stride, r_value = minimize_quadratic_1d(\n",
        "            a, b, r_stride_l, r_stride_u, c=c)\n",
        "        r_h *= r_stride\n",
        "        r_h += p_h\n",
        "        r = r_h * d\n",
        "    else:\n",
        "        r_value = np.inf\n",
        "\n",
        "    # Now correct p_h to make it strictly interior.\n",
        "    p *= theta\n",
        "    p_h *= theta\n",
        "    p_value = evaluate_quadratic(J_h, g_h, p_h, diag=diag_h)\n",
        "\n",
        "    ag_h = -g_h\n",
        "    ag = d * ag_h\n",
        "\n",
        "    to_tr = Delta / norm(ag_h)\n",
        "    to_bound, _ = step_size_to_bound(x, ag, lb, ub)\n",
        "    if to_bound < to_tr:\n",
        "        ag_stride = theta * to_bound\n",
        "    else:\n",
        "        ag_stride = to_tr\n",
        "\n",
        "    a, b = build_quadratic_1d(J_h, g_h, ag_h, diag=diag_h)\n",
        "    ag_stride, ag_value = minimize_quadratic_1d(a, b, 0, ag_stride)\n",
        "    ag_h *= ag_stride\n",
        "    ag *= ag_stride\n",
        "\n",
        "    if p_value < r_value and p_value < ag_value:\n",
        "        return p, p_h, -p_value\n",
        "    elif r_value < p_value and r_value < ag_value:\n",
        "        return r, r_h, -r_value\n",
        "    else:\n",
        "        return ag, ag_h, -ag_value\n",
        "\n",
        "\n",
        "def trf_bounds(fun, jac, x0, f0, J0, lb, ub, ftol, xtol, gtol, max_nfev,\n",
        "               x_scale, loss_function, tr_solver, tr_options, verbose, scale_inv_f):\n",
        "    x = x0.copy()\n",
        "\n",
        "    f = f0\n",
        "    f_true = f.copy()\n",
        "    nfev = 1\n",
        "\n",
        "    J = J0\n",
        "    njev = 1\n",
        "    m, n = J.shape\n",
        "\n",
        "    if loss_function is not None:\n",
        "        rho = loss_function(f)\n",
        "        cost = 0.5 * np.sum(rho[0])\n",
        "        J, f = scale_for_robust_loss_function(J, f, rho)\n",
        "    else:\n",
        "        cost = 0.5 * np.dot(f, f)\n",
        "\n",
        "    g = compute_grad(J, f)\n",
        "\n",
        "    jac_scale = isinstance(x_scale, str) and x_scale == 'jac'\n",
        "    if jac_scale:\n",
        "        scale, scale_inv = compute_jac_scale(np.array(scale_inv_f(x)))\n",
        "    else:\n",
        "        scale, scale_inv = x_scale, 1 / x_scale\n",
        "\n",
        "    v, dv = CL_scaling_vector(x, g, lb, ub)\n",
        "    v[dv != 0] *= scale_inv[dv != 0]\n",
        "    Delta = norm(x0 * scale_inv / v**0.5)\n",
        "    if Delta == 0:\n",
        "        Delta = 1.0\n",
        "\n",
        "    g_norm = norm(g * v, ord=np.inf)\n",
        "\n",
        "    f_augmented = np.zeros((m + n))\n",
        "    if tr_solver == 'exact':\n",
        "        J_augmented = np.empty((m + n, n))\n",
        "    elif tr_solver == 'lsmr':\n",
        "        reg_term = 0.0\n",
        "        regularize = tr_options.pop('regularize', True)\n",
        "\n",
        "    if max_nfev is None:\n",
        "        max_nfev = x0.size * 100\n",
        "\n",
        "    alpha = 0.0  # \"Levenberg-Marquardt\" parameter\n",
        "\n",
        "    termination_status = None\n",
        "    iteration = 0\n",
        "    step_norm = None\n",
        "    actual_reduction = None\n",
        "\n",
        "    if verbose == 2:\n",
        "        print_header_nonlinear()\n",
        "\n",
        "    while True:\n",
        "        v, dv = CL_scaling_vector(x, g, lb, ub)\n",
        "\n",
        "        g_norm = norm(g * v, ord=np.inf)\n",
        "        if g_norm < gtol:\n",
        "            termination_status = 1\n",
        "\n",
        "        if verbose == 2:\n",
        "            print_iteration_nonlinear(iteration, nfev, cost, actual_reduction,\n",
        "                                      step_norm, g_norm)\n",
        "\n",
        "        if termination_status is not None or nfev == max_nfev:\n",
        "            break\n",
        "\n",
        "        # Now compute variables in \"hat\" space. Here, we also account for\n",
        "        # scaling introduced by `x_scale` parameter. This part is a bit tricky,\n",
        "        # you have to write down the formulas and see how the trust-region\n",
        "        # problem is formulated when the two types of scaling are applied.\n",
        "        # The idea is that first we apply `x_scale` and then apply Coleman-Li\n",
        "        # approach in the new variables.\n",
        "\n",
        "        # v is recomputed in the variables after applying `x_scale`, note that\n",
        "        # components which were identically 1 not affected.\n",
        "        v[dv != 0] *= scale_inv[dv != 0]\n",
        "\n",
        "        # Here, we apply two types of scaling.\n",
        "        d = v**0.5 * scale\n",
        "\n",
        "        # C = diag(g * scale) Jv\n",
        "        diag_h = g * dv * scale\n",
        "\n",
        "        # After all this has been done, we continue normally.\n",
        "\n",
        "        # \"hat\" gradient.\n",
        "        g_h = d * g\n",
        "\n",
        "        f_augmented[:m] = f\n",
        "        if tr_solver == 'exact':\n",
        "            J_augmented[:m] = J * d\n",
        "            J_h = J_augmented[:m]  # Memory view.\n",
        "            J_augmented[m:] = np.diag(diag_h**0.5)\n",
        "            U, s, V = svd(J_augmented, full_matrices=False)\n",
        "            V = V.T\n",
        "            uf = U.T.dot(f_augmented)\n",
        "        elif tr_solver == 'lsmr':\n",
        "            J_h = right_multiplied_operator(J, d)\n",
        "\n",
        "            if regularize:\n",
        "                a, b = build_quadratic_1d(J_h, g_h, -g_h, diag=diag_h)\n",
        "                to_tr = Delta / norm(g_h)\n",
        "                ag_value = minimize_quadratic_1d(a, b, 0, to_tr)[1]\n",
        "                reg_term = -ag_value / Delta**2\n",
        "\n",
        "            lsmr_op = regularized_lsq_operator(J_h, (diag_h + reg_term)**0.5)\n",
        "            gn_h = lsmr(lsmr_op, f_augmented, **tr_options)[0]\n",
        "            S = np.vstack((g_h, gn_h)).T\n",
        "            S, _ = qr(S, mode='economic')\n",
        "            JS = J_h.dot(S)  # LinearOperator does dot too.\n",
        "            B_S = np.dot(JS.T, JS) + np.dot(S.T * diag_h, S)\n",
        "            g_S = S.T.dot(g_h)\n",
        "\n",
        "        # theta controls step back step ratio from the bounds.\n",
        "        theta = max(0.995, 1 - g_norm)\n",
        "\n",
        "        actual_reduction = -1\n",
        "        while actual_reduction <= 0 and nfev < max_nfev:\n",
        "            if tr_solver == 'exact':\n",
        "                p_h, alpha, n_iter = solve_lsq_trust_region(\n",
        "                    n, m, uf, s, V, Delta, initial_alpha=alpha)\n",
        "            elif tr_solver == 'lsmr':\n",
        "                p_S, _ = solve_trust_region_2d(B_S, g_S, Delta)\n",
        "                p_h = S.dot(p_S)\n",
        "\n",
        "            p = d * p_h  # Trust-region solution in the original space.\n",
        "            step, step_h, predicted_reduction = select_step(\n",
        "                x, J_h, diag_h, g_h, p, p_h, d, Delta, lb, ub, theta)\n",
        "\n",
        "            x_new = make_strictly_feasible(x + step, lb, ub, rstep=0)\n",
        "            f_new = fun(x_new)\n",
        "            nfev += 1\n",
        "\n",
        "            step_h_norm = norm(step_h)\n",
        "\n",
        "            if not np.all(np.isfinite(f_new)):\n",
        "                Delta = 0.25 * step_h_norm\n",
        "                continue\n",
        "\n",
        "            # Usual trust-region step quality estimation.\n",
        "            if loss_function is not None:\n",
        "                cost_new = loss_function(f_new, cost_only=True)\n",
        "            else:\n",
        "                cost_new = 0.5 * np.dot(f_new, f_new)\n",
        "            actual_reduction = cost - cost_new\n",
        "            Delta_new, ratio = update_tr_radius(\n",
        "                Delta, actual_reduction, predicted_reduction,\n",
        "                step_h_norm, step_h_norm > 0.95 * Delta)\n",
        "\n",
        "            step_norm = norm(step)\n",
        "            termination_status = check_termination(\n",
        "                actual_reduction, cost, step_norm, norm(x), ratio, ftol, xtol)\n",
        "            if termination_status is not None:\n",
        "                break\n",
        "\n",
        "            alpha *= Delta / Delta_new\n",
        "            Delta = Delta_new\n",
        "\n",
        "        if actual_reduction > 0:\n",
        "            x = x_new\n",
        "\n",
        "            f = f_new\n",
        "            f_true = f.copy()\n",
        "\n",
        "            cost = cost_new\n",
        "\n",
        "            J = jac(x, f)\n",
        "            njev += 1\n",
        "\n",
        "            if loss_function is not None:\n",
        "                rho = loss_function(f)\n",
        "                J, f = scale_for_robust_loss_function(J, f, rho)\n",
        "\n",
        "            g = compute_grad(J, f)\n",
        "\n",
        "            if jac_scale:\n",
        "                scale, scale_inv = compute_jac_scale(np.array(scale_inv_f(x)), scale_inv)\n",
        "        else:\n",
        "            step_norm = 0\n",
        "            actual_reduction = 0\n",
        "\n",
        "        iteration += 1\n",
        "\n",
        "    if termination_status is None:\n",
        "        termination_status = 0\n",
        "\n",
        "    active_mask = find_active_constraints(x, lb, ub, rtol=xtol)\n",
        "    return OptimizeResult(\n",
        "        x=x, cost=cost, fun=f_true, jac=J, grad=g, optimality=g_norm,\n",
        "        active_mask=active_mask, nfev=nfev, njev=njev,\n",
        "        status=termination_status)\n",
        "\n",
        "\n",
        "def trf_no_bounds(fun, jac, x0, f0, J0, ftol, xtol, gtol, max_nfev,\n",
        "                  x_scale, loss_function, tr_solver, tr_options, verbose,scale_inv_f):\n",
        "    x = x0.copy()\n",
        "\n",
        "    f = f0\n",
        "    f_true = f.copy()\n",
        "    nfev = 1\n",
        "\n",
        "    J = J0\n",
        "    njev = 1\n",
        "    m, n = J.shape\n",
        "\n",
        "    if loss_function is not None:\n",
        "        rho = loss_function(f)\n",
        "        cost = 0.5 * np.sum(rho[0])\n",
        "        J, f = scale_for_robust_loss_function(J, f, rho)\n",
        "    else:\n",
        "        cost = 0.5 * np.dot(f, f)\n",
        "\n",
        "    g = compute_grad(J, f)\n",
        "\n",
        "    jac_scale = isinstance(x_scale, str) and x_scale == 'jac'\n",
        "    if jac_scale:\n",
        "        scale, scale_inv = compute_jac_scale(np.array(scale_inv_f(x)))\n",
        "    else:\n",
        "        scale, scale_inv = x_scale, 1 / x_scale\n",
        "\n",
        "    Delta = norm(x0 * scale_inv)\n",
        "    if Delta == 0:\n",
        "        Delta = 1.0\n",
        "\n",
        "    if tr_solver == 'lsmr':\n",
        "        reg_term = 0\n",
        "        damp = tr_options.pop('damp', 0.0)\n",
        "        regularize = tr_options.pop('regularize', True)\n",
        "\n",
        "    if max_nfev is None:\n",
        "        max_nfev = x0.size * 100\n",
        "\n",
        "    alpha = 0.0  # \"Levenberg-Marquardt\" parameter\n",
        "\n",
        "    termination_status = None\n",
        "    iteration = 0\n",
        "    step_norm = None\n",
        "    actual_reduction = None\n",
        "\n",
        "    if verbose == 2:\n",
        "        print_header_nonlinear()\n",
        "\n",
        "    while True:\n",
        "        g_norm = norm(g, ord=np.inf)\n",
        "        if g_norm < gtol:\n",
        "            termination_status = 1\n",
        "\n",
        "        if verbose == 2:\n",
        "            print_iteration_nonlinear(iteration, nfev, cost, actual_reduction,\n",
        "                                      step_norm, g_norm)\n",
        "\n",
        "        if termination_status is not None or nfev == max_nfev:\n",
        "            break\n",
        "\n",
        "        d = scale\n",
        "        g_h = d * g\n",
        "\n",
        "        if tr_solver == 'exact':\n",
        "            J_h = J * d\n",
        "            U, s, V = svd(J_h, full_matrices=False)\n",
        "            V = V.T\n",
        "            uf = U.T.dot(f)\n",
        "        elif tr_solver == 'lsmr':\n",
        "            J_h = right_multiplied_operator(J, d)\n",
        "\n",
        "            if regularize:\n",
        "                a, b = build_quadratic_1d(J_h, g_h, -g_h)\n",
        "                to_tr = Delta / norm(g_h)\n",
        "                ag_value = minimize_quadratic_1d(a, b, 0, to_tr)[1]\n",
        "                reg_term = -ag_value / Delta**2\n",
        "\n",
        "            damp_full = (damp**2 + reg_term)**0.5\n",
        "            gn_h = lsmr(J_h, f, damp=damp_full, **tr_options)[0]\n",
        "            S = np.vstack((g_h, gn_h)).T\n",
        "            S, _ = qr(S, mode='economic')\n",
        "            JS = J_h.dot(S)\n",
        "            B_S = np.dot(JS.T, JS)\n",
        "            g_S = S.T.dot(g_h)\n",
        "\n",
        "        actual_reduction = -1\n",
        "        while actual_reduction <= 0 and nfev < max_nfev:\n",
        "            if tr_solver == 'exact':\n",
        "                step_h, alpha, n_iter = solve_lsq_trust_region(\n",
        "                    n, m, uf, s, V, Delta, initial_alpha=alpha)\n",
        "            elif tr_solver == 'lsmr':\n",
        "                p_S, _ = solve_trust_region_2d(B_S, g_S, Delta)\n",
        "                step_h = S.dot(p_S)\n",
        "\n",
        "            predicted_reduction = -evaluate_quadratic(J_h, g_h, step_h)\n",
        "            step = d * step_h\n",
        "            x_new = x + step\n",
        "            f_new = fun(x_new)\n",
        "            nfev += 1\n",
        "\n",
        "            step_h_norm = norm(step_h)\n",
        "\n",
        "            if not np.all(np.isfinite(f_new)):\n",
        "                Delta = 0.25 * step_h_norm\n",
        "                continue\n",
        "\n",
        "            # Usual trust-region step quality estimation.\n",
        "            if loss_function is not None:\n",
        "                cost_new = loss_function(f_new, cost_only=True)\n",
        "            else:\n",
        "                cost_new = 0.5 * np.dot(f_new, f_new)\n",
        "            actual_reduction = cost - cost_new\n",
        "\n",
        "            Delta_new, ratio = update_tr_radius(\n",
        "                Delta, actual_reduction, predicted_reduction,\n",
        "                step_h_norm, step_h_norm > 0.95 * Delta)\n",
        "\n",
        "            step_norm = norm(step)\n",
        "            termination_status = check_termination(\n",
        "                actual_reduction, cost, step_norm, norm(x), ratio, ftol, xtol)\n",
        "            if termination_status is not None:\n",
        "                break\n",
        "\n",
        "            alpha *= Delta / Delta_new\n",
        "            Delta = Delta_new\n",
        "\n",
        "        if actual_reduction > 0:\n",
        "            x = x_new\n",
        "\n",
        "            f = f_new\n",
        "            f_true = f.copy()\n",
        "\n",
        "            cost = cost_new\n",
        "\n",
        "            J = jac(x, f)\n",
        "            njev += 1\n",
        "\n",
        "            if loss_function is not None:\n",
        "                rho = loss_function(f)\n",
        "                J, f = scale_for_robust_loss_function(J, f, rho)\n",
        "\n",
        "            g = compute_grad(J, f)\n",
        "\n",
        "            if jac_scale:\n",
        "                scale, scale_inv = compute_jac_scale(np.array(scale_inv_f(x)), scale_inv)\n",
        "        else:\n",
        "            step_norm = 0\n",
        "            actual_reduction = 0\n",
        "\n",
        "        iteration += 1\n",
        "\n",
        "    if termination_status is None:\n",
        "        termination_status = 0\n",
        "\n",
        "    active_mask = np.zeros_like(x)\n",
        "    return OptimizeResult(\n",
        "        x=x, cost=cost, fun=f_true, jac=J, grad=g, optimality=g_norm,\n",
        "        active_mask=active_mask, nfev=nfev, njev=njev,\n",
        "        status=termination_status)"
      ],
      "metadata": {
        "id": "3TfCDE4LeFC6"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "2bT4vipmd3nJ"
      },
      "outputs": [],
      "source": [
        "\"\"\"Generic interface for least-squares minimization.\"\"\"\n",
        "from warnings import warn\n",
        "\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "from scipy.sparse import issparse, csr_matrix\n",
        "from scipy.sparse.linalg import LinearOperator\n",
        "from scipy.optimize import _minpack, OptimizeResult\n",
        "from scipy.optimize._numdiff import approx_derivative, group_columns\n",
        "\n",
        "from scipy.optimize._lsq.dogbox import dogbox\n",
        "from scipy.optimize._lsq.common import EPS, in_bounds, make_strictly_feasible\n",
        "\n",
        "\n",
        "TERMINATION_MESSAGES = {\n",
        "    -1: \"Improper input parameters status returned from `leastsq`\",\n",
        "    0: \"The maximum number of function evaluations is exceeded.\",\n",
        "    1: \"`gtol` termination condition is satisfied.\",\n",
        "    2: \"`ftol` termination condition is satisfied.\",\n",
        "    3: \"`xtol` termination condition is satisfied.\",\n",
        "    4: \"Both `ftol` and `xtol` termination conditions are satisfied.\"\n",
        "}\n",
        "\n",
        "\n",
        "FROM_MINPACK_TO_COMMON = {\n",
        "    0: -1,  # Improper input parameters from MINPACK.\n",
        "    1: 2,\n",
        "    2: 3,\n",
        "    3: 4,\n",
        "    4: 1,\n",
        "    5: 0\n",
        "    # There are 6, 7, 8 for too small tolerance parameters,\n",
        "    # but we guard against it by checking ftol, xtol, gtol beforehand.\n",
        "}\n",
        "\n",
        "\n",
        "def call_minpack(fun, x0, jac, ftol, xtol, gtol, max_nfev, x_scale, diff_step):\n",
        "    n = x0.size\n",
        "\n",
        "    if diff_step is None:\n",
        "        epsfcn = EPS\n",
        "    else:\n",
        "        epsfcn = diff_step**2\n",
        "\n",
        "    # Compute MINPACK's `diag`, which is inverse of our `x_scale` and\n",
        "    # ``x_scale='jac'`` corresponds to ``diag=None``.\n",
        "    if isinstance(x_scale, str) and x_scale == 'jac':\n",
        "        diag = None\n",
        "    else:\n",
        "        diag = 1 / x_scale\n",
        "\n",
        "    full_output = True\n",
        "    col_deriv = False\n",
        "    factor = 100.0\n",
        "\n",
        "    if jac is None:\n",
        "        if max_nfev is None:\n",
        "            # n squared to account for Jacobian evaluations.\n",
        "            max_nfev = 100 * n * (n + 1)\n",
        "        x, info, status = _minpack._lmdif(\n",
        "            fun, x0, (), full_output, ftol, xtol, gtol,\n",
        "            max_nfev, epsfcn, factor, diag)\n",
        "    else:\n",
        "        if max_nfev is None:\n",
        "            max_nfev = 100 * n\n",
        "        x, info, status = _minpack._lmder(\n",
        "            fun, jac, x0, (), full_output, col_deriv,\n",
        "            ftol, xtol, gtol, max_nfev, factor, diag)\n",
        "\n",
        "    f = info['fvec']\n",
        "\n",
        "    if callable(jac):\n",
        "        J = jac(x)\n",
        "    else:\n",
        "        J = np.atleast_2d(approx_derivative(fun, x))\n",
        "\n",
        "    cost = 0.5 * np.dot(f, f)\n",
        "    g = J.T.dot(f)\n",
        "    g_norm = norm(g, ord=np.inf)\n",
        "\n",
        "    nfev = info['nfev']\n",
        "    njev = info.get('njev', None)\n",
        "\n",
        "    status = FROM_MINPACK_TO_COMMON[status]\n",
        "    active_mask = np.zeros_like(x0, dtype=int)\n",
        "\n",
        "    return OptimizeResult(\n",
        "        x=x, cost=cost, fun=f, jac=J, grad=g, optimality=g_norm,\n",
        "        active_mask=active_mask, nfev=nfev, njev=njev, status=status)\n",
        "\n",
        "\n",
        "def prepare_bounds(bounds, n):\n",
        "    lb, ub = [np.asarray(b, dtype=float) for b in bounds]\n",
        "    if lb.ndim == 0:\n",
        "        lb = np.resize(lb, n)\n",
        "\n",
        "    if ub.ndim == 0:\n",
        "        ub = np.resize(ub, n)\n",
        "\n",
        "    return lb, ub\n",
        "\n",
        "\n",
        "def check_tolerance(ftol, xtol, gtol, method):\n",
        "    def check(tol, name):\n",
        "        if tol is None:\n",
        "            tol = 0\n",
        "        elif tol < EPS:\n",
        "            warn(\"Setting `{}` below the machine epsilon ({:.2e}) effectively \"\n",
        "                 \"disables the corresponding termination condition.\"\n",
        "                 .format(name, EPS))\n",
        "        return tol\n",
        "\n",
        "    ftol = check(ftol, \"ftol\")\n",
        "    xtol = check(xtol, \"xtol\")\n",
        "    gtol = check(gtol, \"gtol\")\n",
        "\n",
        "    if method == \"lm\" and (ftol < EPS or xtol < EPS or gtol < EPS):\n",
        "        raise ValueError(\"All tolerances must be higher than machine epsilon \"\n",
        "                         \"({:.2e}) for method 'lm'.\".format(EPS))\n",
        "    elif ftol < EPS and xtol < EPS and gtol < EPS:\n",
        "        raise ValueError(\"At least one of the tolerances must be higher than \"\n",
        "                         \"machine epsilon ({:.2e}).\".format(EPS))\n",
        "\n",
        "    return ftol, xtol, gtol\n",
        "\n",
        "\n",
        "def check_x_scale(x_scale, x0):\n",
        "    if isinstance(x_scale, str) and x_scale == 'jac':\n",
        "        return x_scale\n",
        "\n",
        "    try:\n",
        "        x_scale = np.asarray(x_scale, dtype=float)\n",
        "        valid = np.all(np.isfinite(x_scale)) and np.all(x_scale > 0)\n",
        "    except (ValueError, TypeError):\n",
        "        valid = False\n",
        "\n",
        "    if not valid:\n",
        "        raise ValueError(\"`x_scale` must be 'jac' or array_like with \"\n",
        "                         \"positive numbers.\")\n",
        "\n",
        "    if x_scale.ndim == 0:\n",
        "        x_scale = np.resize(x_scale, x0.shape)\n",
        "\n",
        "    if x_scale.shape != x0.shape:\n",
        "        raise ValueError(\"Inconsistent shapes between `x_scale` and `x0`.\")\n",
        "\n",
        "    return x_scale\n",
        "\n",
        "\n",
        "def check_jac_sparsity(jac_sparsity, m, n):\n",
        "    if jac_sparsity is None:\n",
        "        return None\n",
        "\n",
        "    if not issparse(jac_sparsity):\n",
        "        jac_sparsity = np.atleast_2d(jac_sparsity)\n",
        "\n",
        "    if jac_sparsity.shape != (m, n):\n",
        "        raise ValueError(\"`jac_sparsity` has wrong shape.\")\n",
        "\n",
        "    return jac_sparsity, group_columns(jac_sparsity)\n",
        "\n",
        "\n",
        "# Loss functions.\n",
        "\n",
        "\n",
        "def huber(z, rho, cost_only):\n",
        "    mask = z <= 1\n",
        "    rho[0, mask] = z[mask]\n",
        "    rho[0, ~mask] = 2 * z[~mask]**0.5 - 1\n",
        "    if cost_only:\n",
        "        return\n",
        "    rho[1, mask] = 1\n",
        "    rho[1, ~mask] = z[~mask]**-0.5\n",
        "    rho[2, mask] = 0\n",
        "    rho[2, ~mask] = -0.5 * z[~mask]**-1.5\n",
        "\n",
        "\n",
        "def soft_l1(z, rho, cost_only):\n",
        "    t = 1 + z\n",
        "    rho[0] = 2 * (t**0.5 - 1)\n",
        "    if cost_only:\n",
        "        return\n",
        "    rho[1] = t**-0.5\n",
        "    rho[2] = -0.5 * t**-1.5\n",
        "\n",
        "\n",
        "def cauchy(z, rho, cost_only):\n",
        "    rho[0] = np.log1p(z)\n",
        "    if cost_only:\n",
        "        return\n",
        "    t = 1 + z\n",
        "    rho[1] = 1 / t\n",
        "    rho[2] = -1 / t**2\n",
        "\n",
        "\n",
        "def arctan(z, rho, cost_only):\n",
        "    rho[0] = np.arctan(z)\n",
        "    if cost_only:\n",
        "        return\n",
        "    t = 1 + z**2\n",
        "    rho[1] = 1 / t\n",
        "    rho[2] = -2 * z / t**2\n",
        "\n",
        "\n",
        "IMPLEMENTED_LOSSES = dict(linear=None, huber=huber, soft_l1=soft_l1,\n",
        "                          cauchy=cauchy, arctan=arctan)\n",
        "\n",
        "\n",
        "def construct_loss_function(m, loss, f_scale):\n",
        "    if loss == 'linear':\n",
        "        return None\n",
        "\n",
        "    if not callable(loss):\n",
        "        loss = IMPLEMENTED_LOSSES[loss]\n",
        "        rho = np.empty((3, m))\n",
        "\n",
        "        def loss_function(f, cost_only=False):\n",
        "            z = (f / f_scale) ** 2\n",
        "            loss(z, rho, cost_only=cost_only)\n",
        "            if cost_only:\n",
        "                return 0.5 * f_scale ** 2 * np.sum(rho[0])\n",
        "            rho[0] *= f_scale ** 2\n",
        "            rho[2] /= f_scale ** 2\n",
        "            return rho\n",
        "    else:\n",
        "        def loss_function(f, cost_only=False):\n",
        "            z = (f / f_scale) ** 2\n",
        "            rho = loss(z)\n",
        "            if cost_only:\n",
        "                return 0.5 * f_scale ** 2 * np.sum(rho[0])\n",
        "            rho[0] *= f_scale ** 2\n",
        "            rho[2] /= f_scale ** 2\n",
        "            return rho\n",
        "\n",
        "    return loss_function\n",
        "\n",
        "\n",
        "def least_squares(\n",
        "        fun, x0, jac='2-point', bounds=(-np.inf, np.inf), method='trf',\n",
        "        ftol=1e-8, xtol=1e-8, gtol=1e-8, x_scale=1.0, loss='linear',\n",
        "        f_scale=1.0, diff_step=None, tr_solver=None, tr_options={},\n",
        "        jac_sparsity=None, max_nfev=None, verbose=0, scale_inv=None, args=(), kwargs={}):\n",
        "    if method not in ['trf', 'dogbox', 'lm']:\n",
        "        raise ValueError(\"`method` must be 'trf', 'dogbox' or 'lm'.\")\n",
        "\n",
        "    if jac not in ['2-point', '3-point', 'cs'] and not callable(jac):\n",
        "        raise ValueError(\"`jac` must be '2-point', '3-point', 'cs' or \"\n",
        "                         \"callable.\")\n",
        "\n",
        "    if tr_solver not in [None, 'exact', 'lsmr']:\n",
        "        raise ValueError(\"`tr_solver` must be None, 'exact' or 'lsmr'.\")\n",
        "\n",
        "    if loss not in IMPLEMENTED_LOSSES and not callable(loss):\n",
        "        raise ValueError(\"`loss` must be one of {0} or a callable.\"\n",
        "                         .format(IMPLEMENTED_LOSSES.keys()))\n",
        "\n",
        "    if method == 'lm' and loss != 'linear':\n",
        "        raise ValueError(\"method='lm' supports only 'linear' loss function.\")\n",
        "\n",
        "    if verbose not in [0, 1, 2]:\n",
        "        raise ValueError(\"`verbose` must be in [0, 1, 2].\")\n",
        "\n",
        "    if len(bounds) != 2:\n",
        "        raise ValueError(\"`bounds` must contain 2 elements.\")\n",
        "\n",
        "    if max_nfev is not None and max_nfev <= 0:\n",
        "        raise ValueError(\"`max_nfev` must be None or positive integer.\")\n",
        "\n",
        "    if np.iscomplexobj(x0):\n",
        "        raise ValueError(\"`x0` must be real.\")\n",
        "\n",
        "    x0 = np.atleast_1d(x0).astype(float)\n",
        "\n",
        "    if x0.ndim > 1:\n",
        "        raise ValueError(\"`x0` must have at most 1 dimension.\")\n",
        "\n",
        "    lb, ub = prepare_bounds(bounds, x0.shape[0])\n",
        "\n",
        "    if method == 'lm' and not np.all((lb == -np.inf) & (ub == np.inf)):\n",
        "        raise ValueError(\"Method 'lm' doesn't support bounds.\")\n",
        "\n",
        "    if lb.shape != x0.shape or ub.shape != x0.shape:\n",
        "        raise ValueError(\"Inconsistent shapes between bounds and `x0`.\")\n",
        "\n",
        "    if np.any(lb >= ub):\n",
        "        raise ValueError(\"Each lower bound must be strictly less than each \"\n",
        "                         \"upper bound.\")\n",
        "\n",
        "    if not in_bounds(x0, lb, ub):\n",
        "        raise ValueError(\"`x0` is infeasible.\")\n",
        "\n",
        "    x_scale = check_x_scale(x_scale, x0)\n",
        "\n",
        "    ftol, xtol, gtol = check_tolerance(ftol, xtol, gtol, method)\n",
        "\n",
        "    def fun_wrapped(x):\n",
        "        return np.atleast_1d(fun(x, *args, **kwargs))\n",
        "\n",
        "    if method == 'trf':\n",
        "        x0 = make_strictly_feasible(x0, lb, ub)\n",
        "\n",
        "    f0 = fun_wrapped(x0)\n",
        "\n",
        "    if f0.ndim != 1:\n",
        "        raise ValueError(\"`fun` must return at most 1-d array_like. \"\n",
        "                         \"f0.shape: {0}\".format(f0.shape))\n",
        "\n",
        "    if not np.all(np.isfinite(f0)):\n",
        "        raise ValueError(\"Residuals are not finite in the initial point.\")\n",
        "\n",
        "    n = x0.size\n",
        "    m = f0.size\n",
        "\n",
        "    if method == 'lm' and m < n:\n",
        "        raise ValueError(\"Method 'lm' doesn't work when the number of \"\n",
        "                         \"residuals is less than the number of variables.\")\n",
        "\n",
        "    loss_function = construct_loss_function(m, loss, f_scale)\n",
        "    if callable(loss):\n",
        "        rho = loss_function(f0)\n",
        "        if rho.shape != (3, m):\n",
        "            raise ValueError(\"The return value of `loss` callable has wrong \"\n",
        "                             \"shape.\")\n",
        "        initial_cost = 0.5 * np.sum(rho[0])\n",
        "    elif loss_function is not None:\n",
        "        initial_cost = loss_function(f0, cost_only=True)\n",
        "    else:\n",
        "        initial_cost = 0.5 * np.dot(f0, f0)\n",
        "\n",
        "    if callable(jac):\n",
        "        J0 = jac(x0, *args, **kwargs)\n",
        "\n",
        "        if issparse(J0):\n",
        "            J0 = J0.tocsr()\n",
        "\n",
        "            def jac_wrapped(x, _=None):\n",
        "                return jac(x, *args, **kwargs).tocsr()\n",
        "\n",
        "        elif isinstance(J0, LinearOperator):\n",
        "            def jac_wrapped(x, _=None):\n",
        "                return jac(x, *args, **kwargs)\n",
        "\n",
        "        else:\n",
        "            J0 = np.atleast_2d(J0)\n",
        "\n",
        "            def jac_wrapped(x, _=None):\n",
        "                return np.atleast_2d(jac(x, *args, **kwargs))\n",
        "\n",
        "    else:  # Estimate Jacobian by finite differences.\n",
        "        if method == 'lm':\n",
        "            if jac_sparsity is not None:\n",
        "                raise ValueError(\"method='lm' does not support \"\n",
        "                                 \"`jac_sparsity`.\")\n",
        "\n",
        "            if jac != '2-point':\n",
        "                warn(\"jac='{0}' works equivalently to '2-point' \"\n",
        "                     \"for method='lm'.\".format(jac))\n",
        "\n",
        "            J0 = jac_wrapped = None\n",
        "        else:\n",
        "            if jac_sparsity is not None and tr_solver == 'exact':\n",
        "                raise ValueError(\"tr_solver='exact' is incompatible \"\n",
        "                                 \"with `jac_sparsity`.\")\n",
        "\n",
        "            jac_sparsity = check_jac_sparsity(jac_sparsity, m, n)\n",
        "\n",
        "            def jac_wrapped(x, f):\n",
        "                J = approx_derivative(fun, x, rel_step=diff_step, method=jac,\n",
        "                                      f0=f, bounds=bounds, args=args,\n",
        "                                      kwargs=kwargs, sparsity=jac_sparsity)\n",
        "                if J.ndim != 2:  # J is guaranteed not sparse.\n",
        "                    J = np.atleast_2d(J)\n",
        "\n",
        "                return J\n",
        "\n",
        "            J0 = jac_wrapped(x0, f0)\n",
        "\n",
        "    if J0 is not None:\n",
        "        if J0.shape != (m, n):\n",
        "            raise ValueError(\n",
        "                \"The return value of `jac` has wrong shape: expected {0}, \"\n",
        "                \"actual {1}.\".format((m, n), J0.shape))\n",
        "\n",
        "        if not isinstance(J0, np.ndarray):\n",
        "            if method == 'lm':\n",
        "                raise ValueError(\"method='lm' works only with dense \"\n",
        "                                 \"Jacobian matrices.\")\n",
        "\n",
        "            if tr_solver == 'exact':\n",
        "                raise ValueError(\n",
        "                    \"tr_solver='exact' works only with dense \"\n",
        "                    \"Jacobian matrices.\")\n",
        "\n",
        "        jac_scale = isinstance(x_scale, str) and x_scale == 'jac'\n",
        "        # if isinstance(J0, LinearOperator) and jac_scale:\n",
        "        #     raise ValueError(\"x_scale='jac' can't be used when `jac` \"\n",
        "        #                      \"returns LinearOperator.\")\n",
        "\n",
        "        if tr_solver is None:\n",
        "            if isinstance(J0, np.ndarray):\n",
        "                tr_solver = 'exact'\n",
        "            else:\n",
        "                tr_solver = 'lsmr'\n",
        "\n",
        "    if method == 'lm':\n",
        "        result = call_minpack(fun_wrapped, x0, jac_wrapped, ftol, xtol, gtol,\n",
        "                              max_nfev, x_scale, diff_step)\n",
        "\n",
        "    elif method == 'trf':\n",
        "        result = trf(fun_wrapped, jac_wrapped, x0, f0, J0, lb, ub, ftol, xtol,\n",
        "                     gtol, max_nfev, x_scale, loss_function, tr_solver,\n",
        "                     tr_options.copy(), verbose, scale_inv)\n",
        "\n",
        "    elif method == 'dogbox':\n",
        "        if tr_solver == 'lsmr' and 'regularize' in tr_options:\n",
        "            warn(\"The keyword 'regularize' in `tr_options` is not relevant \"\n",
        "                 \"for 'dogbox' method.\")\n",
        "            tr_options = tr_options.copy()\n",
        "            del tr_options['regularize']\n",
        "\n",
        "        result = dogbox(fun_wrapped, jac_wrapped, x0, f0, J0, lb, ub, ftol,\n",
        "                        xtol, gtol, max_nfev, x_scale, loss_function,\n",
        "                        tr_solver, tr_options, verbose)\n",
        "\n",
        "    result.message = TERMINATION_MESSAGES[result.status]\n",
        "    result.success = result.status > 0\n",
        "\n",
        "    if verbose >= 1:\n",
        "        print(result.message)\n",
        "        print(\"Function evaluations {0}, initial cost {1:.4e}, final cost \"\n",
        "              \"{2:.4e}, first-order optimality {3:.2e}.\"\n",
        "              .format(result.nfev, initial_cost, result.cost,\n",
        "                      result.optimality))\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "import jax\n",
        "import numpy as np\n",
        "from scipy.sparse.linalg import LinearOperator\n",
        "from jax.config import config\n",
        "from functools import partial\n",
        "config.update(\"jax_enable_x64\", True)"
      ],
      "metadata": {
        "id": "YsHrLduieYeX"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N=1000\n",
        "N_offdiagonal = int(10*N)\n",
        "xguess=jnp.asarray(np.random.uniform(size=N))\n",
        "pairs = np.random.randint(0,N,size=(N_offdiagonal,2))\n",
        "eq = np.random.randint(0,N,N_offdiagonal)\n",
        "coeff = np.random.uniform(-10,10,size=N)"
      ],
      "metadata": {
        "id": "WP-BYym1sNut"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def func(x):\n",
        "    x=jnp.asarray(x)+coeff\n",
        "    res=x**2\n",
        "    res=res.at[eq].add(-x[pairs[:,0]]*x[pairs[:,1]])\n",
        "    return res\n",
        "jit_func = jax.jit(func)\n",
        "\n",
        "def mv(x,v):\n",
        "   return jax.jvp(func,(x,),(jnp.squeeze(v),))[1]\n",
        "\n",
        "jit_mv = jax.jit(mv)\n",
        "\n",
        "def rmv(x,v):\n",
        "    return jax.vjp(func,x)[1](v)\n",
        "\n",
        "jit_rmv = jax.jit(rmv)\n",
        "\n",
        "def LO(x):\n",
        "    return LinearOperator((N,N), matvec = partial(jit_mv,x), rmatvec=partial(jit_rmv, x))\n",
        "\n",
        "def get_norm_f(jit_mv):\n",
        "    v=jnp.zeros_like(xguess)\n",
        "    def norm(x, i):\n",
        "        return x, jnp.linalg.norm(jit_mv(x,v.at[i].set(1)))\n",
        "    return jax.jit(norm)\n",
        "\n",
        "def scale_inv(x):\n",
        "    return jax.lax.scan(get_norm_f(jit_mv),init=x,xs=jnp.arange(xguess.size))[1]\n",
        "\n",
        "scale_inv_jit=jax.jit(scale_inv)\n"
      ],
      "metadata": {
        "id": "rPvWm1F9sW0C"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "sol=least_squares(jit_func,xguess,jac=LO,gtol=1e-12)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfCYYNS9sZPm",
        "outputId": "14694af2-c5ad-4da0-fb95-12738a785c2b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.45 s ± 230 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "sol=least_squares(jit_func,xguess,jac=LO,gtol=1e-12, scale_inv=scale_inv_jit)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5nQrpVCxA_T",
        "outputId": "5ff9b920-0e53-4c79-afc0-be5fde91b9df"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.31 s ± 9.26 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "sol=least_squares(jit_func,xguess,jac=LO,gtol=1e-12, scale_inv=scale_inv_jit, x_scale='jac')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ubyU_kxxhR2",
        "outputId": "7f6599b8-c895-46a9-c963-fb6ef672547c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.01 s ± 46.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ncznntZ31OVn"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "icVSbUTw4GRs"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BDxtTAbP4cpE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}